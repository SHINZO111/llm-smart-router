"""
LM Studio Router Integration Example

router.jsã¨ã®é€£æºç”¨ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
èµ·å‹•æ™‚ã«è‡ªå‹•æ¤œå‡ºã‚’å®Ÿè¡Œã—ã€æ¤œå‡ºã—ãŸãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›ã™ã‚‹
"""

import subprocess
import json
import logging
from pathlib import Path

logger = logging.getLogger(__name__)


def detect_on_startup(config_path="./config.yaml", endpoint="http://localhost:1234/v1"):
    """
    èµ·å‹•æ™‚ã«LM Studioã®ãƒ¢ãƒ‡ãƒ«ã‚’è‡ªå‹•æ¤œå‡º
    
    router.jsã®èµ·å‹•å‡¦ç†ã‹ã‚‰å‘¼ã³å‡ºã™ã“ã¨ã‚’æƒ³å®š
    
    Args:
        config_path: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        endpoint: LM Studio APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
        
    Returns:
        dict: æ¤œå‡ºçµæœ
    """
    try:
        # Pythonãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å®Ÿè¡Œ
        result = subprocess.run(
            [
                "python", "-m", "lmstudio", "detect",
                "--endpoint", endpoint
            ],
            capture_output=True,
            text=True,
            timeout=10
        )
        
        if result.returncode == 0:
            logger.info("âœ… LM Studioãƒ¢ãƒ‡ãƒ«ã‚’æ¤œå‡ºã—ã¾ã—ãŸ")
            logger.info(result.stdout)
            return {"success": True, "output": result.stdout}
        else:
            logger.warning("âš ï¸  LM Studioãƒ¢ãƒ‡ãƒ«ã®æ¤œå‡ºã«å¤±æ•—ã—ã¾ã—ãŸ")
            logger.warning(result.stderr)
            return {"success": False, "error": result.stderr}
            
    except subprocess.TimeoutExpired:
        logger.warning("â±ï¸  LM Studioæ¤œå‡ºãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ")
        return {"success": False, "error": "timeout"}
    except FileNotFoundError:
        logger.warning("ğŸ PythonãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return {"success": False, "error": "python not found"}
    except Exception as e:
        logger.error(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        return {"success": False, "error": str(e)}


def update_config_on_startup(config_path="./config.yaml", endpoint="http://localhost:1234/v1"):
    """
    èµ·å‹•æ™‚ã«LM Studioã®ãƒ¢ãƒ‡ãƒ«ã‚’æ¤œå‡ºã—ã¦config.yamlã‚’æ›´æ–°
    
    Args:
        config_path: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        endpoint: LM Studio APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
        
    Returns:
        dict: æ›´æ–°çµæœ
    """
    try:
        result = subprocess.run(
            [
                "python", "-m", "lmstudio", "update",
                "--config", config_path,
                "--endpoint", endpoint
            ],
            capture_output=True,
            text=True,
            timeout=10
        )
        
        if result.returncode == 0:
            logger.info("âœ… config.yamlã‚’æ›´æ–°ã—ã¾ã—ãŸ")
            logger.info(result.stdout)
            return {"success": True, "output": result.stdout}
        else:
            logger.warning("âš ï¸  config.yamlã®æ›´æ–°ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return {"success": False, "error": result.stderr}
            
    except Exception as e:
        logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        return {"success": False, "error": str(e)}


def get_lmstudio_status(endpoint="http://localhost:1234/v1"):
    """
    LM Studioã®çŠ¶æ…‹ã‚’å–å¾—ï¼ˆJSONå½¢å¼ï¼‰
    
    Returns:
        dict: çŠ¶æ…‹æƒ…å ±
    """
    try:
        from lmstudio.model_detector import LMStudioModelDetector
        
        detector = LMStudioModelDetector(endpoint=endpoint)
        
        if not detector.is_running():
            return {
                "running": False,
                "models": [],
                "default_model": None
            }
        
        models = detector.get_loaded_models()
        default = detector.get_default_model()
        
        return {
            "running": True,
            "models": [m.to_dict() for m in models],
            "default_model": default,
            "model_count": len(models)
        }
        
    except Exception as e:
        return {
            "running": False,
            "error": str(e),
            "models": [],
            "default_model": None
        }


# ç›´æ¥å®Ÿè¡Œæ™‚ã®ãƒ†ã‚¹ãƒˆ
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    print("=" * 60)
    print("LM Studio Router Integration Test")
    print("=" * 60)
    
    # çŠ¶æ…‹ç¢ºèª
    status = get_lmstudio_status()
    print(f"\nLM StudioçŠ¶æ…‹: {'âœ… èµ·å‹•ä¸­' if status['running'] else 'âŒ æœªèµ·å‹•'}")
    
    if status['running']:
        print(f"ãƒ¢ãƒ‡ãƒ«æ•°: {status['model_count']}")
        print(f"ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«: {status['default_model']}")
        print("\næ¤œå‡ºã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«:")
        for model in status['models']:
            print(f"  - {model['id']}")
    
    # æ¤œå‡ºãƒ†ã‚¹ãƒˆ
    print("\n" + "-" * 60)
    print("ãƒ¢ãƒ‡ãƒ«æ¤œå‡ºãƒ†ã‚¹ãƒˆ:")
    print("-" * 60)
    result = detect_on_startup()
    print(result['output'] if result['success'] else result['error'])
